{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MALIS-Project-v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurilaatu/malis/blob/master/MALIS_Project_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OdhRsWKvz6Z",
        "colab_type": "text"
      },
      "source": [
        "## MALIS 2019 EURECOM\n",
        "## Course Project\n",
        "\n",
        "This is version zero of our semester project. We will load and prepare the data. Train a CNN to to extract features and use an ANN or SVM to classify the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAwvC2F8zxf3",
        "colab_type": "text"
      },
      "source": [
        "### Description\n",
        "\n",
        "**Task 1**: **bold text** Project definition for Group 34\n",
        "\tBy:\n",
        "Lauri Laatu\n",
        "Marvin Mouroum\n",
        "\n",
        "\n",
        "**•Context & problem definition**\n",
        "\n",
        "Classifying images is a huge topic in machine learning. The goal for our project is to be able to classify images to multiple classes. More specifically we want to investigate if a simple convolutional neural network is able to classify internet memes correctly. As such our it is a supervised learning problem.\n",
        "\n",
        "\n",
        "**•Methodology**\n",
        "\n",
        "Convolution for feature extraction\n",
        "RGB images will be fetched from a database\n",
        "preprocessing to optimize image dimensions\n",
        "convolution over three color channels of the images\n",
        "exporting a feature vector containing compressed information about the image\n",
        "ANN or SVM for classification based on extracted features\n",
        "feature vectors as labeled input\n",
        "supervised multiclass classification problem\n",
        "dataset usage: initially we plan to start with n-fold cross validation and 10% testing\n",
        "classification in predefined meme categories\n",
        "\n",
        "\n",
        "**•Data**\n",
        "\n",
        "The data will be acquired from the internet and labeled by hand if the data is unlabelled. Goal is to find around a thousand images per class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ao_F4lHwV9D",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "Loading data from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb9dl9KLvstO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import drive as gdrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7KVtVoVkFzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFYxrppMi9Rr",
        "colab_type": "code",
        "outputId": "02c5fbec-afa7-4d5b-ebe3-5bb83fa72f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gdrive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBHkY8fqYsX-",
        "colab_type": "text"
      },
      "source": [
        "### Copy raw pepe images to local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ztr9ThQpGsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir  dataset\n",
        "\n",
        "file_list = drive.ListFile({'q': \"'10DpgRQAxqIU0TLYCruSGPaj3DfRzh6TE' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "root = \"/content/drive/My Drive/MALIS_Project/DataSet/rare-pepes/\"\n",
        "i = 0\n",
        "\n",
        "#!cp \"/content/drive/My Drive/MALIS_Project/DataSet/rare-pepes/001 - OdrldTF.png\" \"dataset/pepe_00.png\"\n",
        "\n",
        "for file in file_list:\n",
        "  target = \"'dataset/pepe_\"+str(i)+\".png'\"\n",
        "  src    = \"'\"+root+file['title']+\"'\"\n",
        "  #print(src)\n",
        "  #print(target)\n",
        "  !cp $src $target\n",
        "  i+=1\n",
        "\n",
        "!ls -1 dataset\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wnbd0JY920",
        "colab_type": "text"
      },
      "source": [
        "### Resizing Image so it can be used by the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGp94evi40y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_img(img,destination):\n",
        "  # Create square images from pepes by adding black margins preserving original aspect ratio\n",
        "  #Importing modules opencv + numpy\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "\n",
        "  #Reading an image (you can use PNG or JPG)\n",
        "  img = cv2.imread(img)\n",
        "\n",
        "  if img is None:\n",
        "    return False\n",
        "\n",
        "  #Getting the bigger side of the image\n",
        "  s = max(img.shape[0:2])\n",
        "\n",
        "  #Creating a dark square with NUMPY  \n",
        "  f = np.zeros((s,s,3),np.uint8)\n",
        "\n",
        "  #Getting the centering position\n",
        "  ax,ay = (s - img.shape[1])//2,(s - img.shape[0])//2\n",
        "\n",
        "  #Pasting the 'image' in a centering position\n",
        "  f[ay:img.shape[0]+ay,ax:ax+img.shape[1]] = img\n",
        "\n",
        "  #Showing results (just in case) \n",
        "  #cv2.imshow(\"IMG\",f)\n",
        "  #A pause, waiting for any press in keyboard\n",
        "  #cv2.waitKey(0)\n",
        "\n",
        "  #Saving the image\n",
        "  f = cv2.resize(f,(227,227),interpolation=cv2.INTER_AREA)\n",
        "  cv2.imwrite(destination,f)\n",
        "  cv2.destroyAllWindows() \n",
        "  return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGit1f3klTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir resized\n",
        "for i in range(0,1273):\n",
        "  path =   \"dataset/pepe_\" + str(i) + \".png\"\n",
        "  _path = \"'resized/pepe_\" + str(i) + \".png'\"\n",
        "  prev_dest = \"resized/pepe_\" + str(i) + \".png\"\n",
        "  destination = \"'/content/drive/My Drive/MALIS_Project/DataSet/resized_pepe/pepe_\" + str(i) + \".png'\"\n",
        "  if not resize_img(path,prev_dest):\n",
        "    print(\"error with\", i)\n",
        "  !cp $_path $destination \n",
        "  if(i%50 == 0):\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXLKPrOFZI5d",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the Not Pepe Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sb4-Gew4pG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_list = drive.ListFile({'q': \"'1UBhhkryHE302KDQ8ufDvkfv7vozMZfli' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "!mkdir dataset\n",
        "\n",
        "root = \"/content/drive/My Drive/MALIS_Project/DataSet/not-pepes/\"\n",
        "i = 0\n",
        "rootfolder = \"'\"+root+\"'\"\n",
        "\n",
        "#!cp \"/content/drive/My Drive/MALIS_Project/DataSet/rare-pepes/001 - OdrldTF.png\" \"dataset/pepe_00.png\"\n",
        "\n",
        "for file in file_list:\n",
        "  target = \"dataset/not_pepe_\"+str(i)+\".\"+file['title'].split('.')[-1]\n",
        "  src    =  root+file['title']\n",
        "  _src   = \"'\"+target+\"'\"  \n",
        "  _target = \"'/content/drive/My Drive/MALIS_Project/DataSet/resized_not_pepe/not_pepe_\"+str(i)+\".\"+file['title'].split('.')[-1]+\"'\"\n",
        "  \n",
        "  if not resize_img(src,target):\n",
        "    print(\"error with\", i)\n",
        "    continue\n",
        "  !cp $_src $_target\n",
        "  i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBz62dkL_RkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive/MALIS_Project/DataSet/not-pepes/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTM1sxvQ-ZE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbXUkmjPaV3x",
        "colab_type": "text"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeXGDCN9EcUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import math #for calculus\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50UNpv9Uaea6",
        "colab_type": "text"
      },
      "source": [
        "Defining the Network class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5MNItVfabDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PepeNN(nn.Module):   \n",
        "    \n",
        "    def __init__(self,name='PepeNN'):\n",
        "        super(PepeNN, self).__init__()\n",
        "        self.T = 120\n",
        "        \n",
        "        self.name = name\n",
        "        \n",
        "        self.training_epochs = 0\n",
        "        \n",
        "        self.lr_history       = []\n",
        "        self.accuracy_history = []\n",
        "        self.loss_history     = []\n",
        "        \n",
        "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "        #torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,20,  kernel_size=(11,11),stride=(4,4),padding=(1,1),dilation=(1,1))\n",
        "        self.batchnorm1 = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20,40,kernel_size=(5,5),stride=(2,2),padding=(1,1),dilation=(1,1))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(40)\n",
        "\n",
        "        self.pooling1 = nn.MaxPool2d( kernel_size=(5,5), stride=(2,2), padding=(1,1), dilation=(1,1))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(40,20,kernel_size=(3,3),stride=(2,2),padding=(1,1),dilation=(1,1))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(20)\n",
        "        self.conv4 = nn.Conv2d(20,10,kernel_size=(2,2),stride=(1,1),padding=(0,0),dilation=(1,1))\n",
        "        self.batchnorm4 = nn.BatchNorm2d(10)\n",
        "\n",
        "        self.pooling2 = nn.MaxPool2d(kernel_size=(2,2), stride=(1,1), padding=(0,0), dilation=(1,1))\n",
        "        \n",
        "        #fully connected at the end\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(in_features=250, out_features=2)\n",
        "        #self.fc1 = torch.nn.Linear(in_features=250, out_features=125)\n",
        "        #self.fc2 = torch.nn.Linear(in_features=125, out_features=2)\n",
        "    \n",
        "        \n",
        "        # robust weight initialization\n",
        "        torch.nn.init.xavier_normal_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv3.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv4.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        #torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "        self.SVM = svm.LinearSVC()\n",
        "        self.svm_train_input = []\n",
        "        self.svm_label_input = []\n",
        "        self.svm_active = False\n",
        "        self.SVM.max_iter = 10000\n",
        "        self.SVM.C = 0.01\n",
        "      \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        batchSize = x.shape[0]\n",
        "\n",
        "        # first set of CNNs and then a max pool\n",
        "        #print(\"starting with \" + str(x.shape))\n",
        "        x = self.conv1(x)\n",
        "       # print(\"after 1 \" + str(x.shape))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        #print(\"after 2 \" + str(x.shape))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pooling1(x)\n",
        "        #print(\"after 3 \" + str(x.shape))\n",
        "        \n",
        "        # second set\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        #print(\"after 4 \" + str(x.shape))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        #print(\"after 5 \" + str(x.shape))\n",
        "        x = self.batchnorm4(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pooling2(x)\n",
        "        #print(\"after 6 \" + str(x.shape))\n",
        "        mean = x.mean()\n",
        "        std = x.std()\n",
        "        svm_input = (x - mean)/std\n",
        "        res = []\n",
        "\n",
        "        if self.svm_active:\n",
        "          if len(self.svm_train_input) == 0:\n",
        "            self.svm_train_input.append(svm_input)\n",
        "          else:\n",
        "\n",
        "            if self.svm_train_input[0].ndim == 5:\n",
        "              n = self.svm_train_input[0].shape[0]\n",
        "              b = self.svm_train_input[0].shape[1]\n",
        "              h = self.svm_train_input[0].shape[2]\n",
        "              w = self.svm_train_input[0].shape[3]\n",
        "              d = self.svm_train_input[0].shape[4]\n",
        "\n",
        "              self.svm_train_input[0] = torch.cat([self.svm_train_input[0], svm_input.reshape([1,b,h,w,d])], dim=0)\n",
        "            else:\n",
        "              self.svm_train_input[0] = torch.stack([self.svm_train_input[0], svm_input], dim=0)\n",
        "\n",
        "        else:\n",
        "          b,h,w,d = svm_input.shape\n",
        "          predict_input = svm_input.clone().detach().cpu().reshape(b,h*w*d).tolist()\n",
        "          res = self.SVM.predict(predict_input)\n",
        "\n",
        "        x= self.fc1(x.reshape((batchSize,250)))\n",
        "        #x= self.fc2(x)\n",
        "\n",
        "        x= F.softmax(x,dim=1)\n",
        "        \n",
        "        return x, res\n",
        "      \n",
        "    def save(self,root='gdrive/My\\ Drive/MALIS_Project'):\n",
        "      !mkdir $root\n",
        "  \n",
        "      path = root.replace('\\\\','') + '/' + self.name + '.pickle'\n",
        "    \n",
        "      print(\"\\nsaving network under:\\n\",path)\n",
        "      \n",
        "      with open(path, 'wb') as f:\n",
        "        pickle.dump(self, f)\n",
        "        \n",
        "    def next_epoch(self,lr,acc_hist,loss_hist):\n",
        "      self.training_epochs += 1\n",
        "      self.lr_history.append(lr)\n",
        "      self.accuracy_history.append(acc_hist)\n",
        "      self.loss_history.append(loss_hist)\n",
        "    \n",
        "    def reset(self):\n",
        "      self.training_epochs = 0\n",
        "      self.lr_history = []\n",
        "      self.accuracy_history = []\n",
        "      self.loss_history = []\n",
        "\n",
        "    def train_svm(self):\n",
        "      print(len(self.svm_train_input))\n",
        "      print(len(self.svm_label_input))\n",
        "      print(self.svm_train_input[0].shape)\n",
        "      print(self.svm_label_input[0].shape)\n",
        "\n",
        "      inputs = []\n",
        "      labels = []\n",
        "\n",
        "      i_clone = self.svm_train_input[0].clone().detach().cpu().numpy()\n",
        "      i_clone = i_clone.reshape(i_clone.shape[0]*i_clone.shape[1],i_clone.shape[2]*i_clone.shape[3]*i_clone.shape[4])\n",
        "      inputs = i_clone.tolist()\n",
        "\n",
        "      l_clone = self.svm_label_input[0].clone().detach().cpu().numpy() \n",
        "      l_clone = l_clone.reshape(l_clone.shape[0]*l_clone.shape[1])\n",
        "      labels = l_clone.tolist()\n",
        "\n",
        "\n",
        "      print(len(inputs))\n",
        "      print(len(labels))\n",
        "\n",
        "      print(inputs[0])\n",
        "      print(labels[0])\n",
        "\n",
        "      self.SVM.fit(inputs, labels)\n",
        "\n",
        "    def add_svm_labels(self,labels):\n",
        "      if len(self.svm_label_input) == 0:\n",
        "        #print(\"first append svm training label\")\n",
        "        self.svm_label_input.append(labels)\n",
        "      else:\n",
        "\n",
        "        if self.svm_label_input[0].ndim == 2:\n",
        "          n = self.svm_label_input[0].shape[0]\n",
        "          b = self.svm_label_input[0].shape[1]\n",
        "\n",
        "          #print(self.svm_label_input[0].shape)\n",
        "          #print(labels.reshape([1,b]).shape)\n",
        "\n",
        "          self.svm_label_input[0] = torch.cat([self.svm_label_input[0], labels.reshape([1,b])], dim=0)\n",
        "        else:\n",
        "          #print(self.svm_label_input[0].shape)\n",
        "          #print(labels.shape)\n",
        "          self.svm_label_input[0] = torch.stack([self.svm_label_input[0], labels], dim=0)\n",
        "\n",
        "        \n",
        "      #self.svm_label_input.append(labels)\n",
        "    \n",
        "    def newEpoch(self):\n",
        "      self.svm_train_input.clear()\n",
        "      self.svm_label_input.clear()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk9bhf32ln3p",
        "colab_type": "text"
      },
      "source": [
        "Creating n folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puwj74yslnCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!ls \"/content/drive/My Drive/MALIS_Project/DataSet/\"\n",
        "\n",
        "\n",
        "# id for Dataset folder 1ugHotUpAsw1o-V8JvN7JwEXWNv-tTp-i\n",
        "\n",
        "file_list = drive.ListFile(\n",
        "    {'q': \"'1ugHotUpAsw1o-V8JvN7JwEXWNv-tTp-i' in parents\"}).GetList()\n",
        "\n",
        "# create an array of tuples with (id, label, fold,file)\n",
        "\n",
        "dataset = []\n",
        "npepes = []\n",
        "\n",
        "pepes = []\n",
        "\n",
        "for i in file_list:\n",
        "  if \"resized\" in i['title']:\n",
        "    if \"not\" in i['title']:\n",
        "      print(i)\n",
        "      for notpepe in drive.ListFile({'q': \"'%s' in parents\"%(i['id']) }).GetList():\n",
        "        # \n",
        "        npepes.append((len(dataset), 0, None, notpepe))\n",
        "      #print(i)\n",
        "    else:\n",
        "      for pepe in drive.ListFile({'q': \"'%s' in parents\"%(i['id']) }).GetList():\n",
        "        # \n",
        "        pepes.append((len(dataset), 1, None, pepe))      \n",
        "      #print(i)\n",
        "  \n",
        "\n",
        "print(len(npepes),len(pepes))\n",
        "\n",
        "folds = 6\n",
        "for fold in range(folds):\n",
        "  foldsize = int(len(npepes)/folds)\n",
        "  for i in range(fold*foldsize, (fold+1)*foldsize):\n",
        "    npepes[i] = (npepes[i][0], npepes[i][1], fold, npepes[i][3])\n",
        "\n",
        "for i in range(len(npepes)-1,1, -1):\n",
        "  if npepes[i][2] is None:\n",
        "    npepes[i] = (npepes[i][0], npepes[i][1], folds-1, npepes[i][3])\n",
        "  else:\n",
        "    break\n",
        "\n",
        "for fold in range(folds):\n",
        "  foldsize = int(len(pepes)/folds)\n",
        "  for i in range(fold*foldsize, (fold+1)*foldsize):\n",
        "    pepes[i] = (pepes[i][0], pepes[i][1], fold, pepes[i][3])\n",
        "\n",
        "for i in range(len(pepes)-1,1, -1):\n",
        "  if pepes[i][2] is None:\n",
        "    pepes[i] = (pepes[i][0], pepes[i][1], folds-1, pepes[i][3])\n",
        "  else:\n",
        "    break\n",
        "\n",
        "for x in npepes[-5:]:\n",
        "  print(x)\n",
        "\n",
        "for x in pepes[-5:]:\n",
        "  print(x)\n",
        "\n",
        "# now we have labeled dataset partitioned into n number of folds  \n",
        "dataset = npepes+pepes\n",
        "print(len(dataset))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "#print(file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGDCDqR7FMFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset[3000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmFwctamHtzl",
        "colab_type": "text"
      },
      "source": [
        "### Methods for training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jwSjc5aHxnk",
        "colab_type": "text"
      },
      "source": [
        "Some methods..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5EpK0xrFNi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cost_function():\n",
        "    \n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "    \n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  #optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=wd, amsgrad=False)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JXJxifHI0nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net,t_data,t_labels, cost_function, device='cuda:0'):\n",
        "  \n",
        "  net.eval()\n",
        "  net.svm_active = False\n",
        "\n",
        "  cumulative_loss = 0\n",
        "  accuracy = 0\n",
        "  svm_accuracy = 0\n",
        "\n",
        "  samples = t_data.shape[0]\n",
        "  \n",
        "  with torch.no_grad():\n",
        "\n",
        "    mean = t_data.mean()\n",
        "    std  = t_data.std()\n",
        "      \n",
        "    t_data = (t_data - mean)/std\n",
        "      \n",
        "    # Load data into GPU\n",
        "    inputs = torch.LongTensor(t_data).float().to(device)\n",
        "    targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs, svm_outputs = net(inputs)\n",
        "      \n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    #print(\"loss \" +str(loss))\n",
        "    cumulative_loss += loss.item()\n",
        "    #print(\"cim loss \" +str(cumulative_loss))\n",
        "    _, predicted = outputs.max(1)\n",
        "    #print(\"ourputs -> \" , outputs)\n",
        "    #print(\"predicted -> \" , predicted)\n",
        "    #print(\"targets   -> \" , targets)\n",
        "    accuracy += predicted.eq(targets).sum().item()/samples\n",
        "    svm_accuracy += torch.LongTensor(svm_outputs).eq(targets.clone().cpu().detach()).sum().item()/samples\n",
        "    #print(\"accuracy \" + str(accuracy))\n",
        "\n",
        "  return cumulative_loss, accuracy, svm_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFkgvofjr1c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net,t_data,t_labels,optimizer,cost_function, device='cuda:0'):\n",
        "  net.train()\n",
        "  net.svm_active = True\n",
        "\n",
        "  cumulative_loss = 0\n",
        "  accuracy = 0\n",
        "\n",
        "  samples = t_data.shape[0]\n",
        "\n",
        "  mean = t_data.mean()\n",
        "  std  = t_data.std()\n",
        "      \n",
        "  t_data = (t_data - mean)/std\n",
        "      \n",
        "  # Load data into GPU\n",
        "  inputs = torch.LongTensor(t_data).float().to(device)\n",
        "  targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "  # Forward pass\n",
        "  outputs, svm_inputs = net(inputs)\n",
        "  net.add_svm_labels(targets)\n",
        "      \n",
        "  # Apply the loss\n",
        "  loss = cost_function(outputs, targets)\n",
        "\n",
        "  # Backward pass\n",
        "  loss.backward()\n",
        "    \n",
        "  # Update parameters\n",
        "  optimizer.step()\n",
        "    \n",
        "  # Reset the optimizer\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #print(\"loss \" +str(loss))\n",
        "  cumulative_loss += loss.item()\n",
        "  #print(\"cim loss \" +str(cumulative_loss))\n",
        "  _, predicted = outputs.max(1)\n",
        "  #print(\"ourputs -> \" , outputs)\n",
        "  #print(\"predicted -> \" , predicted)\n",
        "  #print(\"targets   -> \" , targets)\n",
        "  accuracy += predicted.eq(targets).sum().item()/samples\n",
        "  #print(\"accuracy \" + str(accuracy))\n",
        "\n",
        "  return cumulative_loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75JLrl2Am6W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl79OeARKlOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "#dataset[3000][1] -> label\n",
        "#dataset[3000][2] -> n fold\n",
        "\n",
        "\n",
        "first_fold = list(filter(lambda x: x[2] == 0, dataset))\n",
        "second_fold = list(filter(lambda x: x[2] == 1, dataset))\n",
        "third_fold = list(filter(lambda x: x[2] == 2, dataset))\n",
        "fourth_fold = list(filter(lambda x: x[2] == 3, dataset))\n",
        "fifth_fold = list(filter(lambda x: x[2] == 4, dataset))\n",
        "#print(len(first_fold))\n",
        "\n",
        "trainingData = [first_fold,second_fold,third_fold,fourth_fold,fifth_fold]\n",
        "\n",
        "def getTrainingData(n_fold):\n",
        "\n",
        "  shuffle(n_fold)\n",
        "\n",
        "  images  = list(map(lambda x: x[3], n_fold))\n",
        "  classes  = list(map(lambda x: x[2], n_fold))\n",
        "  labels = list(map(lambda x: x[1], n_fold))\n",
        "\n",
        "  imageArray = np.zeros((len(images),227,227,3))\n",
        "\n",
        "  for i in range(0,len(images)):\n",
        "    standard = 'drive/My Drive/'\n",
        "    root = standard + 'MALIS_Project/DataSet/'\n",
        "\n",
        "    if (labels[i] == 0):\n",
        "      root = root + 'resized_not_pepe/'\n",
        "\n",
        "    else:\n",
        "      root = root + 'resized_pepe/'\n",
        "\n",
        "    filepath = root + images[i]['title']\n",
        "    image = cv2.imread(filepath) #numpy array\n",
        "    if image.size == 0:\n",
        "      print(\"empty image\")\n",
        "    imageArray[i] = image\n",
        "\n",
        "  return imageArray, labels\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvyamaUUuxEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = PepeNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tZiGA_KLB8t",
        "colab_type": "code",
        "outputId": "ecba1c0e-c305-4308-c6d6-abb01d541ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "device='cuda:0'\n",
        "net.to(device)\n",
        "\n",
        "testLoss = 0\n",
        "trainLoss = 0\n",
        "total_acc = 0\n",
        "counter = 0\n",
        "\n",
        "max_epochs = 10\n",
        "lr = 0.001\n",
        "\n",
        "for e in range(0,max_epochs):\n",
        "  \n",
        "  if counter > 0 and total_acc/counter > 0.99:\n",
        "    print(\"accuracy high enough aborting\")\n",
        "    break\n",
        "\n",
        "  print(\"On epoch \", e)\n",
        "  net.newEpoch()\n",
        "\n",
        "  for _n in range(0,4):\n",
        "    imageArray, labels = getTrainingData(trainingData[_n])\n",
        "    a=0\n",
        "    n=20\n",
        "    N= imageArray.shape[0]\n",
        "\n",
        "    intervall = n-a\n",
        "\n",
        "    for i in range(0,N,intervall):\n",
        "    \n",
        "      a = i\n",
        "      n = i + intervall\n",
        "\n",
        "      if n >= N:\n",
        "        continue\n",
        "\n",
        "      t_data   = np.transpose(imageArray[a:n].reshape((n-a,227,227,3)), (0,3, 1, 2))\n",
        "      t_labels = np.array([labels[a:n]]).reshape((n-a,))\n",
        "\n",
        "      if t_data.size == 0:\n",
        "        print(\"skip\")\n",
        "        continue\n",
        "\n",
        "      #tLoss, accuracy =test(net,t_data,t_labels,get_cost_function(),device)\n",
        "      #testLoss += tLoss\n",
        "\n",
        "      optimizer = get_optimizer(net, lr=lr, wd=1e-6, momentum=0.9)\n",
        "\n",
        "      Loss, accuracy = train(net,t_data,t_labels,optimizer,get_cost_function(),device)\n",
        "      trainLoss += Loss\n",
        "      #print(\"accumulated loss = \" +str(trainLoss))\n",
        "      #print(\"accuracy -> \" + str(accuracy))\n",
        "      total_acc += accuracy\n",
        "      counter += 1\n",
        "\n",
        "    print(\"\\nNEXT FOLD\\n\")\n",
        "  print(\"accuracy \",total_acc/counter)\n",
        "    \n",
        "net.train_svm()\n"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On epoch  0\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8037037037037036\n",
            "On epoch  1\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8421296296296292\n",
            "On epoch  2\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8618827160493812\n",
            "On epoch  3\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8761574074074043\n",
            "On epoch  4\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8853703703703667\n",
            "On epoch  5\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.8941358024691338\n",
            "On epoch  6\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.9002645502645503\n",
            "On epoch  7\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.9059027777777797\n",
            "On epoch  8\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.9103395061728425\n",
            "On epoch  9\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "\n",
            "NEXT FOLD\n",
            "\n",
            "accuracy  0.9140277777777818\n",
            "1\n",
            "1\n",
            "torch.Size([108, 20, 10, 5, 5])\n",
            "torch.Size([108, 20])\n",
            "2160\n",
            "2160\n",
            "[-0.9765424728393555, -0.8643355965614319, -0.8643355965614319, -0.9765424728393555, 0.5006853938102722, -0.545567512512207, -0.545567512512207, -0.2760240435600281, -0.2760240435600281, 0.15311512351036072, 1.640899419784546, 1.640899419784546, 0.529654860496521, 1.6624277830123901, 1.6624277830123901, 1.640899419784546, 1.640899419784546, 0.529654860496521, 1.6624277830123901, 1.6624277830123901, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.4361077845096588, -0.8905236124992371, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.3327436149120331, -0.00736685935407877, -0.07248390465974808, -0.9765424728393555, -0.9765424728393555, -0.5676108598709106, 0.4245870113372803, -0.07248390465974808, -0.9765424728393555, -0.9765424728393555, 0.07498587667942047, 0.5247576236724854, -0.3901652693748474, -0.3901652693748474, 0.17859961092472076, 0.8328937888145447, 0.5247576236724854, -0.3901652693748474, -0.3901652693748474, 0.17859961092472076, 0.8328937888145447, -0.9765424728393555, -0.3393014371395111, 0.44239258766174316, 0.44239258766174316, 0.1936148703098297, -0.9765424728393555, -0.3393014371395111, 0.44239258766174316, 0.44239258766174316, 0.1936148703098297, -0.9765424728393555, -0.8865140080451965, -0.4094010591506958, -0.4042394757270813, -0.03236611932516098, -0.4022354185581207, -0.4022354185581207, -0.9223077297210693, -0.46032387018203735, -0.46032387018203735, 2.65989089012146, 2.65989089012146, 2.3227007389068604, 2.3227007389068604, 2.1910672187805176, -0.7469857335090637, 1.9422882795333862, 1.9422882795333862, 0.7697317600250244, 2.8268167972564697, -0.7469857335090637, 1.9422882795333862, 1.9422882795333862, 1.4337990283966064, 2.8268167972564697, -0.9097834825515747, 1.481062412261963, 1.481062412261963, 1.4337990283966064, 1.4337990283966064, 0.6662477254867554, 0.6662477254867554, 1.7240804433822632, 1.7240804433822632, 1.8335225582122803, 2.883002996444702, 3.3467376232147217, 3.3467376232147217, 3.100996732711792, 2.4947564601898193, 0.2577165961265564, 0.2577165961265564, -0.5383058190345764, -0.48817816376686096, -0.21819999814033508, -0.6747402548789978, 0.54726243019104, 0.54726243019104, -0.9635730385780334, -0.9765424728393555, 0.6230522394180298, 0.838740885257721, 1.6253292560577393, 1.6253292560577393, 1.2748703956604004, 0.6230522394180298, 0.838740885257721, 1.6253292560577393, 1.6253292560577393, 1.2748703956604004, -0.9765424728393555, -0.9765424728393555, -0.8871153593063354, -0.40558773279190063, -0.20870228111743927, -0.4821726679801941, -0.4821726679801941, -0.410645067691803, -0.2419099658727646, -0.2419099658727646, -0.5920502543449402, -0.5920502543449402, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.8803293704986572, 0.9849299192428589, 1.795206904411316, 1.795206904411316, 0.9585879445075989, -0.5473552942276001, 0.9849299192428589, 1.795206904411316, 1.795206904411316, 0.9585879445075989, 0.18881388008594513, -0.26090478897094727, -0.18836097419261932, -0.18836097419261932, -0.27845561504364014, -0.9765424728393555, -0.9765424728393555, -0.9462180733680725, -0.3789195716381073, 0.09199301153421402, 0.47028419375419617, 0.47028419375419617, -0.19339004158973694, -0.14128714799880981, 0.09199301153421402, 0.47028419375419617, 0.47028419375419617, -0.19339004158973694, -0.14128714799880981, -0.14128714799880981, -0.9027819633483887, -0.9027819633483887, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, 5.762313365936279, 5.762313365936279, 5.229263782501221, 4.726877212524414, 4.612278938293457, -0.9765424728393555, -0.8445345163345337, -0.35597118735313416, -0.35597118735313416, -0.36689144372940063, -0.31180065870285034, -0.31180065870285034, 0.6583252549171448, 0.6583252549171448, -0.22007085382938385, -0.31180065870285034, -0.31180065870285034, 0.6583252549171448, 0.6583252549171448, -0.12226704508066177, -0.9765424728393555, -0.9765424728393555, -0.20271919667720795, 0.03314684331417084, 0.03314684331417084, -0.4497321546077728, -0.4497321546077728, -0.19883903861045837, 0.1077556163072586, 0.1077556163072586, 1.1679198741912842, 1.169934868812561, 1.4743577241897583, 1.4743577241897583, 0.3503119945526123, 1.1679198741912842, 1.169934868812561, 1.4743577241897583, 1.4743577241897583, 0.6319351196289062, -0.8848394751548767, -0.8848394751548767, -0.39801859855651855, -0.39801859855651855, 0.6319351196289062, -0.4194921553134918, -0.5252510905265808, -0.1710442304611206, -0.1710442304611206, -0.43888118863105774, -0.4194921553134918, -0.5252510905265808, -0.1710442304611206, -0.1710442304611206, -0.4257330000400543, 1.7657394409179688, 1.7657394409179688, 1.1830397844314575, 1.000001072883606, 2.136329412460327, -0.32680681347846985, 0.11278928071260452, 0.11278928071260452, -0.40634438395500183, -0.40634438395500183, -0.5126481056213379, 0.5215898752212524, 0.5215898752212524, 0.1944887638092041, 0.1944887638092041, -0.5126481056213379, 0.5215898752212524, 0.5215898752212524, 0.1944887638092041, 0.1944887638092041, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4Sw0adqDtnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "6faf85db-31ed-46d0-f3db-543b1e042cea"
      },
      "source": [
        "net.SVM.C = 0.01\n",
        "net.train_svm()"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "torch.Size([108, 20, 10, 5, 5])\n",
            "torch.Size([108, 20])\n",
            "2160\n",
            "2160\n",
            "[-0.9765424728393555, -0.8643355965614319, -0.8643355965614319, -0.9765424728393555, 0.5006853938102722, -0.545567512512207, -0.545567512512207, -0.2760240435600281, -0.2760240435600281, 0.15311512351036072, 1.640899419784546, 1.640899419784546, 0.529654860496521, 1.6624277830123901, 1.6624277830123901, 1.640899419784546, 1.640899419784546, 0.529654860496521, 1.6624277830123901, 1.6624277830123901, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.4361077845096588, -0.8905236124992371, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.3327436149120331, -0.00736685935407877, -0.07248390465974808, -0.9765424728393555, -0.9765424728393555, -0.5676108598709106, 0.4245870113372803, -0.07248390465974808, -0.9765424728393555, -0.9765424728393555, 0.07498587667942047, 0.5247576236724854, -0.3901652693748474, -0.3901652693748474, 0.17859961092472076, 0.8328937888145447, 0.5247576236724854, -0.3901652693748474, -0.3901652693748474, 0.17859961092472076, 0.8328937888145447, -0.9765424728393555, -0.3393014371395111, 0.44239258766174316, 0.44239258766174316, 0.1936148703098297, -0.9765424728393555, -0.3393014371395111, 0.44239258766174316, 0.44239258766174316, 0.1936148703098297, -0.9765424728393555, -0.8865140080451965, -0.4094010591506958, -0.4042394757270813, -0.03236611932516098, -0.4022354185581207, -0.4022354185581207, -0.9223077297210693, -0.46032387018203735, -0.46032387018203735, 2.65989089012146, 2.65989089012146, 2.3227007389068604, 2.3227007389068604, 2.1910672187805176, -0.7469857335090637, 1.9422882795333862, 1.9422882795333862, 0.7697317600250244, 2.8268167972564697, -0.7469857335090637, 1.9422882795333862, 1.9422882795333862, 1.4337990283966064, 2.8268167972564697, -0.9097834825515747, 1.481062412261963, 1.481062412261963, 1.4337990283966064, 1.4337990283966064, 0.6662477254867554, 0.6662477254867554, 1.7240804433822632, 1.7240804433822632, 1.8335225582122803, 2.883002996444702, 3.3467376232147217, 3.3467376232147217, 3.100996732711792, 2.4947564601898193, 0.2577165961265564, 0.2577165961265564, -0.5383058190345764, -0.48817816376686096, -0.21819999814033508, -0.6747402548789978, 0.54726243019104, 0.54726243019104, -0.9635730385780334, -0.9765424728393555, 0.6230522394180298, 0.838740885257721, 1.6253292560577393, 1.6253292560577393, 1.2748703956604004, 0.6230522394180298, 0.838740885257721, 1.6253292560577393, 1.6253292560577393, 1.2748703956604004, -0.9765424728393555, -0.9765424728393555, -0.8871153593063354, -0.40558773279190063, -0.20870228111743927, -0.4821726679801941, -0.4821726679801941, -0.410645067691803, -0.2419099658727646, -0.2419099658727646, -0.5920502543449402, -0.5920502543449402, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.8803293704986572, 0.9849299192428589, 1.795206904411316, 1.795206904411316, 0.9585879445075989, -0.5473552942276001, 0.9849299192428589, 1.795206904411316, 1.795206904411316, 0.9585879445075989, 0.18881388008594513, -0.26090478897094727, -0.18836097419261932, -0.18836097419261932, -0.27845561504364014, -0.9765424728393555, -0.9765424728393555, -0.9462180733680725, -0.3789195716381073, 0.09199301153421402, 0.47028419375419617, 0.47028419375419617, -0.19339004158973694, -0.14128714799880981, 0.09199301153421402, 0.47028419375419617, 0.47028419375419617, -0.19339004158973694, -0.14128714799880981, -0.14128714799880981, -0.9027819633483887, -0.9027819633483887, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, 5.762313365936279, 5.762313365936279, 5.229263782501221, 4.726877212524414, 4.612278938293457, -0.9765424728393555, -0.8445345163345337, -0.35597118735313416, -0.35597118735313416, -0.36689144372940063, -0.31180065870285034, -0.31180065870285034, 0.6583252549171448, 0.6583252549171448, -0.22007085382938385, -0.31180065870285034, -0.31180065870285034, 0.6583252549171448, 0.6583252549171448, -0.12226704508066177, -0.9765424728393555, -0.9765424728393555, -0.20271919667720795, 0.03314684331417084, 0.03314684331417084, -0.4497321546077728, -0.4497321546077728, -0.19883903861045837, 0.1077556163072586, 0.1077556163072586, 1.1679198741912842, 1.169934868812561, 1.4743577241897583, 1.4743577241897583, 0.3503119945526123, 1.1679198741912842, 1.169934868812561, 1.4743577241897583, 1.4743577241897583, 0.6319351196289062, -0.8848394751548767, -0.8848394751548767, -0.39801859855651855, -0.39801859855651855, 0.6319351196289062, -0.4194921553134918, -0.5252510905265808, -0.1710442304611206, -0.1710442304611206, -0.43888118863105774, -0.4194921553134918, -0.5252510905265808, -0.1710442304611206, -0.1710442304611206, -0.4257330000400543, 1.7657394409179688, 1.7657394409179688, 1.1830397844314575, 1.000001072883606, 2.136329412460327, -0.32680681347846985, 0.11278928071260452, 0.11278928071260452, -0.40634438395500183, -0.40634438395500183, -0.5126481056213379, 0.5215898752212524, 0.5215898752212524, 0.1944887638092041, 0.1944887638092041, -0.5126481056213379, 0.5215898752212524, 0.5215898752212524, 0.1944887638092041, 0.1944887638092041, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555, -0.9765424728393555]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxGaJpJCSpny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imageArray, labels = getTrainingData(trainingData[4])\n",
        "N= imageArray.shape[0]\n",
        "tot_acc = 0\n",
        "tot_svm_acc = 0\n",
        "samples = 0\n",
        "\n",
        "for i in range(0,N,20):\n",
        "  a = i\n",
        "  n = i + 20\n",
        "  if n > N:\n",
        "    break\n",
        "#  print(\"testing on iteration \" , i+1)\n",
        "  t_data   = np.transpose(imageArray[a:n].reshape((n-a,227,227,3)), (0,3, 1, 2))\n",
        "  t_labels = np.array([labels[a:n]]).reshape((n-a,))\n",
        "  tLoss, taccuracy, svm_acc = test(net,t_data,t_labels,get_cost_function(),device)\n",
        "  tot_acc += taccuracy\n",
        "  tot_svm_acc += svm_acc\n",
        "  samples += 1\n",
        "\n",
        "tot_acc = tot_acc/samples\n",
        "tot_svm_acc = tot_svm_acc/samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JkJ6z9_WCth",
        "colab_type": "code",
        "outputId": "fc6e2c68-da7d-42dd-bb8e-6f9c5e4abc49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tot_acc)\n",
        "print(tot_svm_acc)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9203703703703702\n",
            "0.9388888888888887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBynrnpMvaJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db2-qbz3fYfG",
        "colab_type": "text"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YukTUU10fozl",
        "colab_type": "text"
      },
      "source": [
        "## Performance\n",
        "\n",
        "-1-\n",
        "\n",
        "learning rate: **0.01**\n",
        "\n",
        "epochs: **8**\n",
        "\n",
        "train accuracy: **0.85**\n",
        "\n",
        "test accuracy: **0.95**\n",
        "\n",
        "-2-\n",
        "\n",
        "learning rate: **0.001**\n",
        "\n",
        "epochs: **12**\n",
        "\n",
        "train accuracy: **0.91**\n",
        "\n",
        "test accuracy: **0.9**\n",
        "\n",
        "-3-\n",
        "\n",
        "learning rate: **0.0001**\n",
        "\n",
        "epochs: **16**\n",
        "\n",
        "train accuracy: **0.97**\n",
        "\n",
        "test accuracy: **0.9**\n"
      ]
    }
  ]
}